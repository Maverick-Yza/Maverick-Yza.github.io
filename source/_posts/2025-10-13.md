---
title: 2025.10.13
date: 2025-10-13 10:25:54
tags:
---
# Overview
1. 十一回顾
2. 思考与摘录
3. 今日学习笔记
4. 迎新晚会感受

## 十一回顾
1. Day1：
   1. 上午到达杭州，在办理入住前在大厅里学习了一会儿
   2. 在办理完入住放好东西后去逛了杭州图书馆
   3. 之后去了天目里逛了一圈并在茑屋书店呆了一会儿
   4. 逛了五柳巷历史街区
   5. 前往胜利河美食街吃完饭并买了防晒防虫喷雾
2. Day2：
   1. 早起坐车前往九溪烟树，走了九溪十八涧
   2. 中午前到达龙井村，被一家热情人家拉进去喝了一会儿龙井茶，聊天的过程中了解了龙井茶主要是采摘的春茶，且根据采摘的时间不同品质也有所区分，可分为明前茶、雨前茶和雨后茶，且采摘完的茶还要炒三遍需要花费一个多小时
   3. 在吃过午饭和买了点龙井茶后开始顺着十里锒铛开始爬茶山，一路直至云栖竹径出山
   4. 晚上前往参加Anson Seabra的live house
3. Day3：
   1. 凌晨4点起床骑车前往西湖，到了后沿着北山街一路骑行欣赏夜景
   2. 于5点在神舟基地景点坐等日出
   3. 看完日出后骑行到西湖东南角开始环湖步行
   4. 用了不到3小时换西湖一圈
   5. 晚上前往城市阳台观看灯光秀
4. Day4：
   1. 早起前往灵隐寺，并把整个景区逛完
   2. 骑车前往茅家埠，并徒步沿着湖边的小道把西湖西面逛完
   3. 乘坐了水上公交7号线
   4. 去了钟书阁
5. Day5：
   1. 参加了西湖边的国庆音乐喷泉
   2. 买了伴手礼
6. Day6：
   1. 去到了拱宸桥，逛了博物馆，并沿着河道一路观赏
   2. 乘坐了水上公交1号线
   3. 去了顾雪岩旧居，并一路穿过街巷直至西湖边
7. Day7:
   1. 一整天在杭州宋城景区游玩
8. Day8:
   1. 逛了良渚古城遗址公园
   2. 去了玉鸟集文艺街区
9. Day9：
    1. 逛了西溪湿地
10. Day10：
    1. 去了临安的青山湖，绕湖骑行一圈并参观了水上森林
    2. 下午再次去到茑屋书店用一下午的时间看完了《通往夏天的隧道，再见的出口》一书

由于最后一天晚上吃了一家过辣的饭导致接下来几天只能躺在宿舍


## 思考与摘录

思考：在旅行最后一下午我决心一直待在书店里看完一本书，最终总算在关店前看完了我此生最快速看完的一本书，我其实原本对看书就不是特别上心，虽然时常在脑海里闪过要多看书的念头，也常常去书店寻找自己感兴趣的书，但读着读着最后总会不了了之，所以这次我下定决心当天不看完不走，第一次快速完整的看完一本书真的很开心，而且其实之前就看过了这本书翻拍的电影，但这次读过原著后我深深体会到电影还是没办法很好展现书中的细节和打动人的地方的，所以经此转折我将对读书更进一步，我发现最近我读书的速度和耐心明显提升了，我终会将看书变成由衷热爱的事情  
其次在旅行的其中一天我看到了一个让我收获很多的视频，视频采访了某个行业大佬，使我决定要将生活中的自己记录下来，我打算尝试拍点视频，而且也会在此后的博客文章中加入更多平日里的所思所想，这既是对自己的激励，也希望对日后读到这些内容的人们有所帮助

文案摘录：
1. 心灯不借他人火，自照乾坤步步明，人生如逆旅，你我皆行人，唯有不断悦己、阅己、越己，才能活出生命的真正意义
2. 所有不尊重你的人，赌的都是你没有前途，他们赌你会忍，赌你会忘，赌你就算记得也没有本事反抗。所以你一定要记住你来时路的苦楚，待到来年春暖花开时，愿你安睡时山河入梦，醒来时满目春风，愿你快乐事有始有终，愿你未来路坦荡从容
3. 大仁不仁


## 学习笔记
### 数据科学（统计）
机器学习 -- 让计算机无需显式编程即可获得学习能力的研究领域  
1. 与传统编程的区别：
   1. 传统编程：数据 + 预设程序（函数） --通过程序中的函数进行计算--> 基于计算得到结果  
   2. 机器学习：数据 + 期望输出  --通过曲线拟合（如线性回归）等方式推导程序--> 能预测新数据的程序（模型）  
2. 机器学习的两种学习方式：
   1. 记忆（memorization）：仅积累个体事实，限制于“观察事实的时间”和“储存事实的记忆空间”，无法应对未见过的情况
   2. 泛化（generalization）：从已有的事实推导新事实，本质是一种预测活动，核心假设是“过去可以预测未来”，限制于“推导过程的准确性”，**是机器学习的核心**
3. 机器学习的基本步骤：
   1. 观察训练数据：获取用于学习的样本集合
   2. 推断数据生成过程：通过算法（如线性回归拟合多项式曲线）分析训练数据，提炼数据背后的规律
   3. 预测测试数据：利用推断出的规律，对未见过的样本（测试数据）进行预测
4. 机器学习的核心范式：监督学习与无监督学习
   1. 监督学习（supervised）：
      1. 核心输入：包含 “特征 / 标签对” 的数据集，标签明确指示样本的类别
      2. 核心目标：找到能预测标签的规则，为未见过的输入（仅特征）分配正确标签。
      3. 关键任务：分类（classification）
         1. 目标：在特征空间中找到分隔不同标签组的 “分类面”（如 2D 空间中的直线、高维空间中的平面）
         2. 约束：需控制分类面复杂度，避免过拟合（若分类面过于复杂，会精准匹配训练数据但无法适应测试数据）
         3. 权衡：当标签组存在重叠时，需平衡 “假阳性”（将负类误判为正类）与 “假阴性”（将正类误判为负类）
      4. 案例：足球队员位置分类
   2. 无监督学习（unsupervised）
      1. 核心输入：仅包含特征向量的数据集，无任何标签信息
      2. 核心目标：将数据自动分组为 “自然簇”（具有相似特征的样本集合），或为不同簇创建标签
      3. 关键任务：聚类（clustering）
         1. 聚类步骤：
            1. 随机选择 k 个样本作为 “原型”（初始簇中心）
            2. 计算剩余样本与各原型的距离，将样本归入距离最近的簇（最小化簇内样本距离，即优化目标函数）
            3. 计算每个簇的中值样本，将其作为新原型
            4. 重复步骤 2-3，直至原型不再变化
         2. 关键指标：相似度（通过距离度量衡量，距离越小相似度越高）
      4. 案例：足球队员身高体重聚类
5. 机器学习方法的关键要素
   1. 训练数据与评估方法
      1. 数据划分：需将数据集随机分为 “训练集”（用于学习模型）与 “测试集”（用于评估模型泛化能力），避免用训练数据直接评估（易高估性能）
      2. 评估逻辑：通过模型在测试集上的预测结果，判断模型是否能适应未见过的数据
   2. 特征表示与特征工程
      1. 核心观点：“所有模型都是错的，但有些是有用的”，特征的质量决定模型的有用性
      2. 特征工程目标：构建 “高信噪比（SNR）” 的特征向量 —— 最大化 “有用输入”占比，最小化 “无关输入”占比，避免过拟合
      3. 案例：爬行动物分类特征优化
   3. 距离度量（Distance Metric）
      1. 常用度量：闵可夫斯基度量
         1. 当 p=1 时：曼哈顿距离，计算各维度差值的绝对值之和，适用于维度不可比的场景
         2. 当 p=2 时：欧氏距离，计算各维度差值的平方和的平方根，是最常用的度量方式。
      2. 案例：动物距离计算对比
      3. 关键注意事项：需统一特征维度的权重，避免某一维度（如整数型腿数）对距离计算产生过度影响
   4. 目标函数与约束
      1. 目标函数：定义模型的优化方向（如聚类中 “最小化簇内样本距离”，分类中 “最大化分类面与样本的距离”）
      2. 约束条件：限制模型复杂度（如聚类中指定簇数 k，分类中限制分类面为直线而非复杂曲线），避免过拟合
   5. 优化方法：用于求解目标函数的算法，如聚类中 “更新簇中值为新原型”、分类中 “寻找最优分类面” 的梯度下降算法等
6. **本节知识的重点问题**
   1. 监督学习与无监督学习的核心差异是什么？在实际应用中如何选择这两种范式？
      1. 二者的核心差异体现在数据要求与核心目标上：
         1. 数据要求：监督学习需 “特征 / 标签对”，无监督学习仅需无标签特征向量
         2. 核心目标：监督学习目标是构建分类器，为新输入预测标签；无监督学习目标是将数据聚类为自然簇，挖掘隐含分组
      2. 实际应用选择依据：
         1. 若有明确的标签数据，且需预测新样本标签，选择监督学习
         2. 若无标签数据，仅需探索数据内在结构，或为后续监督学习生成初始标签，选择无监督学习
   2. 特征工程在机器学习中扮演什么角色？如何通过特征工程提升模型性能？
      1. 特征工程的核心角色是构建高信噪比（SNR）的特征向量，即筛选 “有用输入”、剔除 “无关输入”，直接决定模型能否从数据中学习到有效规律
      2. 提升模型性能的方式：
         1. 保留有用特征：选择与目标强相关的特征
         2. 剔除无关特征：移除与目标无关的特征
         3. 优化特征格式：统一特征维度权重（如将整数型 “腿数” 转为二进制，避免距离计算偏差）
   3. 什么是过拟合？为什么会出现过拟合？如何通过模型设计或评估方式避免过拟合？
      1. 过拟合定义：模型在训练数据上表现极佳，但在测试数据上表现差，即模型过度学习训练数据的噪声，而非普遍规律，无法泛化到新数据
      2. 过拟合原因：
         1. 模型复杂度过高
         2. 特征质量差
         3. 仅用训练数据评估模型
      3. 避免方式：
         1. 控制模型复杂度：如分类时选择简单分类面（直线 / 平面），聚类时限制簇数 k（而非 k 等于样本数）
         2. 优化特征工程：剔除无关特征，提升信噪比
         3. 合理划分数据与评估：将数据分为训练集与测试集，仅通过测试集性能判断模型好坏（如复杂模型训练准确率高但测试准确率低，需放弃）
         4. 权衡假阳性与假阴性：避免为追求训练准确率而构建过度复杂的模型

    
## 迎新晚会总结
今天的表演还算是成功吧，和上一次相比明显放松多了，而且这次的听众也挺多，再接再厉吧。